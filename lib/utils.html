<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>2_pytorch_finetune.lib.utils API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>2_pytorch_finetune.lib.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from __future__ import print_function

from collections import defaultdict, deque
import datetime
import pickle
import time

import torch
import torch.distributed as dist

import errno
import os


class SmoothedValue(object):
    &#34;&#34;&#34;Track a series of values and provide access to smoothed values over a
    window or the global series average.
    &#34;&#34;&#34;

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = &#34;{median:.4f} ({global_avg:.4f})&#34;
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        &#34;&#34;&#34;
        Warning: does not synchronize the deque!
        &#34;&#34;&#34;
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=&#39;cuda&#39;)
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value)


def all_gather(data):
    &#34;&#34;&#34;
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    &#34;&#34;&#34;
    world_size = get_world_size()
    if world_size == 1:
        return [data]

    # serialized to a Tensor
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to(&#34;cuda&#34;)

    # obtain Tensor size of each rank
    local_size = torch.tensor([tensor.numel()], device=&#34;cuda&#34;)
    size_list = [torch.tensor([0], device=&#34;cuda&#34;) for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)

    # receiving Tensor from all ranks
    # we pad the tensor because torch all_gather does not support
    # gathering tensors of different shapes
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=&#34;cuda&#34;))
    if local_size != max_size:
        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=&#34;cuda&#34;)
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)

    data_list = []
    for size, tensor in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))

    return data_list


def reduce_dict(input_dict, average=True):
    &#34;&#34;&#34;
    Args:
        input_dict (dict): all the values will be reduced
        average (bool): whether to do average or sum
    Reduce the values in the dictionary from all processes so that all processes
    have the averaged results. Returns a dict with the same fields as
    input_dict, after reduction.
    &#34;&#34;&#34;
    world_size = get_world_size()
    if world_size &lt; 2:
        return input_dict
    with torch.no_grad():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.all_reduce(values)
        if average:
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict


class MetricLogger(object):
    def __init__(self, delimiter=&#34;\t&#34;):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(&#34;&#39;{}&#39; object has no attribute &#39;{}&#39;&#34;.format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(
                &#34;{}: {}&#34;.format(name, str(meter))
            )
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = &#39;&#39;
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
        data_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
        space_fmt = &#39;:&#39; + str(len(str(len(iterable)))) + &#39;d&#39;
        if torch.cuda.is_available():
            log_msg = self.delimiter.join([
                header,
                &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
                &#39;eta: {eta}&#39;,
                &#39;{meters}&#39;,
                &#39;time: {time}&#39;,
                &#39;data: {data}&#39;,
                &#39;max mem: {memory:.0f}&#39;
            ])
        else:
            log_msg = self.delimiter.join([
                header,
                &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
                &#39;eta: {eta}&#39;,
                &#39;{meters}&#39;,
                &#39;time: {time}&#39;,
                &#39;data: {data}&#39;
            ])
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time),
                        memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print(&#39;{} Total time: {} ({:.4f} s / it)&#39;.format(
            header, total_time_str, total_time / len(iterable)))


def collate_fn(batch):
    return tuple(zip(*batch))


def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):

    def f(x):
        if x &gt;= warmup_iters:
            return 1
        alpha = float(x) / warmup_iters
        return warmup_factor * (1 - alpha) + alpha

    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)


def mkdir(path):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise


def setup_for_distributed(is_master):
    &#34;&#34;&#34;
    This function disables printing when not in master process
    &#34;&#34;&#34;
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop(&#39;force&#39;, False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print


def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True


def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()


def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()


def is_main_process():
    return get_rank() == 0


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


def init_distributed_mode(args):
    if &#39;RANK&#39; in os.environ and &#39;WORLD_SIZE&#39; in os.environ:
        args.rank = int(os.environ[&#34;RANK&#34;])
        args.world_size = int(os.environ[&#39;WORLD_SIZE&#39;])
        args.gpu = int(os.environ[&#39;LOCAL_RANK&#39;])
    elif &#39;SLURM_PROCID&#39; in os.environ:
        args.rank = int(os.environ[&#39;SLURM_PROCID&#39;])
        args.gpu = args.rank % torch.cuda.device_count()
    else:
        print(&#39;Not using distributed mode&#39;)
        args.distributed = False
        return

    args.distributed = True

    torch.cuda.set_device(args.gpu)
    args.dist_backend = &#39;nccl&#39;
    print(&#39;| distributed init (rank {}): {}&#39;.format(
        args.rank, args.dist_url), flush=True)
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                         world_size=args.world_size, rank=args.rank)
    torch.distributed.barrier()
    setup_for_distributed(args.rank == 0)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="2_pytorch_finetune.lib.utils.all_gather"><code class="name flex">
<span>def <span class="ident">all_gather</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>Run all_gather on arbitrary picklable data (not necessarily tensors)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>any picklable object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[data]</code></dt>
<dd>list of data gathered from each rank</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_gather(data):
    &#34;&#34;&#34;
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    &#34;&#34;&#34;
    world_size = get_world_size()
    if world_size == 1:
        return [data]

    # serialized to a Tensor
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to(&#34;cuda&#34;)

    # obtain Tensor size of each rank
    local_size = torch.tensor([tensor.numel()], device=&#34;cuda&#34;)
    size_list = [torch.tensor([0], device=&#34;cuda&#34;) for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)

    # receiving Tensor from all ranks
    # we pad the tensor because torch all_gather does not support
    # gathering tensors of different shapes
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=&#34;cuda&#34;))
    if local_size != max_size:
        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=&#34;cuda&#34;)
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)

    data_list = []
    for size, tensor in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))

    return data_list</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.collate_fn"><code class="name flex">
<span>def <span class="ident">collate_fn</span></span>(<span>batch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collate_fn(batch):
    return tuple(zip(*batch))</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.get_rank"><code class="name flex">
<span>def <span class="ident">get_rank</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.get_world_size"><code class="name flex">
<span>def <span class="ident">get_world_size</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.init_distributed_mode"><code class="name flex">
<span>def <span class="ident">init_distributed_mode</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_distributed_mode(args):
    if &#39;RANK&#39; in os.environ and &#39;WORLD_SIZE&#39; in os.environ:
        args.rank = int(os.environ[&#34;RANK&#34;])
        args.world_size = int(os.environ[&#39;WORLD_SIZE&#39;])
        args.gpu = int(os.environ[&#39;LOCAL_RANK&#39;])
    elif &#39;SLURM_PROCID&#39; in os.environ:
        args.rank = int(os.environ[&#39;SLURM_PROCID&#39;])
        args.gpu = args.rank % torch.cuda.device_count()
    else:
        print(&#39;Not using distributed mode&#39;)
        args.distributed = False
        return

    args.distributed = True

    torch.cuda.set_device(args.gpu)
    args.dist_backend = &#39;nccl&#39;
    print(&#39;| distributed init (rank {}): {}&#39;.format(
        args.rank, args.dist_url), flush=True)
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                         world_size=args.world_size, rank=args.rank)
    torch.distributed.barrier()
    setup_for_distributed(args.rank == 0)</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.is_dist_avail_and_initialized"><code class="name flex">
<span>def <span class="ident">is_dist_avail_and_initialized</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.is_main_process"><code class="name flex">
<span>def <span class="ident">is_main_process</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_main_process():
    return get_rank() == 0</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.mkdir"><code class="name flex">
<span>def <span class="ident">mkdir</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mkdir(path):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.reduce_dict"><code class="name flex">
<span>def <span class="ident">reduce_dict</span></span>(<span>input_dict, average=True)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>all the values will be reduced</dd>
<dt><strong><code>average</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to do average or sum</dd>
</dl>
<p>Reduce the values in the dictionary from all processes so that all processes
have the averaged results. Returns a dict with the same fields as
input_dict, after reduction.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_dict(input_dict, average=True):
    &#34;&#34;&#34;
    Args:
        input_dict (dict): all the values will be reduced
        average (bool): whether to do average or sum
    Reduce the values in the dictionary from all processes so that all processes
    have the averaged results. Returns a dict with the same fields as
    input_dict, after reduction.
    &#34;&#34;&#34;
    world_size = get_world_size()
    if world_size &lt; 2:
        return input_dict
    with torch.no_grad():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.all_reduce(values)
        if average:
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.save_on_master"><code class="name flex">
<span>def <span class="ident">save_on_master</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.setup_for_distributed"><code class="name flex">
<span>def <span class="ident">setup_for_distributed</span></span>(<span>is_master)</span>
</code></dt>
<dd>
<div class="desc"><p>This function disables printing when not in master process</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_for_distributed(is_master):
    &#34;&#34;&#34;
    This function disables printing when not in master process
    &#34;&#34;&#34;
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop(&#39;force&#39;, False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.warmup_lr_scheduler"><code class="name flex">
<span>def <span class="ident">warmup_lr_scheduler</span></span>(<span>optimizer, warmup_iters, warmup_factor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):

    def f(x):
        if x &gt;= warmup_iters:
            return 1
        alpha = float(x) / warmup_iters
        return warmup_factor * (1 - alpha) + alpha

    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="2_pytorch_finetune.lib.utils.MetricLogger"><code class="flex name class">
<span>class <span class="ident">MetricLogger</span></span>
<span>(</span><span>delimiter='\t')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MetricLogger(object):
    def __init__(self, delimiter=&#34;\t&#34;):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(&#34;&#39;{}&#39; object has no attribute &#39;{}&#39;&#34;.format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(
                &#34;{}: {}&#34;.format(name, str(meter))
            )
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = &#39;&#39;
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
        data_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
        space_fmt = &#39;:&#39; + str(len(str(len(iterable)))) + &#39;d&#39;
        if torch.cuda.is_available():
            log_msg = self.delimiter.join([
                header,
                &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
                &#39;eta: {eta}&#39;,
                &#39;{meters}&#39;,
                &#39;time: {time}&#39;,
                &#39;data: {data}&#39;,
                &#39;max mem: {memory:.0f}&#39;
            ])
        else:
            log_msg = self.delimiter.join([
                header,
                &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
                &#39;eta: {eta}&#39;,
                &#39;{meters}&#39;,
                &#39;time: {time}&#39;,
                &#39;data: {data}&#39;
            ])
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time),
                        memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print(&#39;{} Total time: {} ({:.4f} s / it)&#39;.format(
            header, total_time_str, total_time / len(iterable)))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="2_pytorch_finetune.lib.utils.MetricLogger.add_meter"><code class="name flex">
<span>def <span class="ident">add_meter</span></span>(<span>self, name, meter)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_meter(self, name, meter):
    self.meters[name] = meter</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.MetricLogger.log_every"><code class="name flex">
<span>def <span class="ident">log_every</span></span>(<span>self, iterable, print_freq, header=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_every(self, iterable, print_freq, header=None):
    i = 0
    if not header:
        header = &#39;&#39;
    start_time = time.time()
    end = time.time()
    iter_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
    data_time = SmoothedValue(fmt=&#39;{avg:.4f}&#39;)
    space_fmt = &#39;:&#39; + str(len(str(len(iterable)))) + &#39;d&#39;
    if torch.cuda.is_available():
        log_msg = self.delimiter.join([
            header,
            &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
            &#39;eta: {eta}&#39;,
            &#39;{meters}&#39;,
            &#39;time: {time}&#39;,
            &#39;data: {data}&#39;,
            &#39;max mem: {memory:.0f}&#39;
        ])
    else:
        log_msg = self.delimiter.join([
            header,
            &#39;[{0&#39; + space_fmt + &#39;}/{1}]&#39;,
            &#39;eta: {eta}&#39;,
            &#39;{meters}&#39;,
            &#39;time: {time}&#39;,
            &#39;data: {data}&#39;
        ])
    MB = 1024.0 * 1024.0
    for obj in iterable:
        data_time.update(time.time() - end)
        yield obj
        iter_time.update(time.time() - end)
        if i % print_freq == 0 or i == len(iterable) - 1:
            eta_seconds = iter_time.global_avg * (len(iterable) - i)
            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
            if torch.cuda.is_available():
                print(log_msg.format(
                    i, len(iterable), eta=eta_string,
                    meters=str(self),
                    time=str(iter_time), data=str(data_time),
                    memory=torch.cuda.max_memory_allocated() / MB))
            else:
                print(log_msg.format(
                    i, len(iterable), eta=eta_string,
                    meters=str(self),
                    time=str(iter_time), data=str(data_time)))
        i += 1
        end = time.time()
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print(&#39;{} Total time: {} ({:.4f} s / it)&#39;.format(
        header, total_time_str, total_time / len(iterable)))</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.MetricLogger.synchronize_between_processes"><code class="name flex">
<span>def <span class="ident">synchronize_between_processes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def synchronize_between_processes(self):
    for meter in self.meters.values():
        meter.synchronize_between_processes()</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.MetricLogger.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, **kwargs):
    for k, v in kwargs.items():
        if isinstance(v, torch.Tensor):
            v = v.item()
        assert isinstance(v, (float, int))
        self.meters[k].update(v)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue"><code class="flex name class">
<span>class <span class="ident">SmoothedValue</span></span>
<span>(</span><span>window_size=20, fmt=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Track a series of values and provide access to smoothed values over a
window or the global series average.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SmoothedValue(object):
    &#34;&#34;&#34;Track a series of values and provide access to smoothed values over a
    window or the global series average.
    &#34;&#34;&#34;

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = &#34;{median:.4f} ({global_avg:.4f})&#34;
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        &#34;&#34;&#34;
        Warning: does not synchronize the deque!
        &#34;&#34;&#34;
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=&#39;cuda&#39;)
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.avg"><code class="name">var <span class="ident">avg</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def avg(self):
    d = torch.tensor(list(self.deque), dtype=torch.float32)
    return d.mean().item()</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.global_avg"><code class="name">var <span class="ident">global_avg</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def global_avg(self):
    return self.total / self.count</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.max"><code class="name">var <span class="ident">max</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def max(self):
    return max(self.deque)</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.median"><code class="name">var <span class="ident">median</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def median(self):
    d = torch.tensor(list(self.deque))
    return d.median().item()</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.value"><code class="name">var <span class="ident">value</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value(self):
    return self.deque[-1]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.synchronize_between_processes"><code class="name flex">
<span>def <span class="ident">synchronize_between_processes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Warning: does not synchronize the deque!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def synchronize_between_processes(self):
    &#34;&#34;&#34;
    Warning: does not synchronize the deque!
    &#34;&#34;&#34;
    if not is_dist_avail_and_initialized():
        return
    t = torch.tensor([self.count, self.total], dtype=torch.float64, device=&#39;cuda&#39;)
    dist.barrier()
    dist.all_reduce(t)
    t = t.tolist()
    self.count = int(t[0])
    self.total = t[1]</code></pre>
</details>
</dd>
<dt id="2_pytorch_finetune.lib.utils.SmoothedValue.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, value, n=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, value, n=1):
    self.deque.append(value)
    self.count += n
    self.total += value * n</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="2_pytorch_finetune.lib" href="index.html">2_pytorch_finetune.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="2_pytorch_finetune.lib.utils.all_gather" href="#2_pytorch_finetune.lib.utils.all_gather">all_gather</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.collate_fn" href="#2_pytorch_finetune.lib.utils.collate_fn">collate_fn</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.get_rank" href="#2_pytorch_finetune.lib.utils.get_rank">get_rank</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.get_world_size" href="#2_pytorch_finetune.lib.utils.get_world_size">get_world_size</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.init_distributed_mode" href="#2_pytorch_finetune.lib.utils.init_distributed_mode">init_distributed_mode</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.is_dist_avail_and_initialized" href="#2_pytorch_finetune.lib.utils.is_dist_avail_and_initialized">is_dist_avail_and_initialized</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.is_main_process" href="#2_pytorch_finetune.lib.utils.is_main_process">is_main_process</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.mkdir" href="#2_pytorch_finetune.lib.utils.mkdir">mkdir</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.reduce_dict" href="#2_pytorch_finetune.lib.utils.reduce_dict">reduce_dict</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.save_on_master" href="#2_pytorch_finetune.lib.utils.save_on_master">save_on_master</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.setup_for_distributed" href="#2_pytorch_finetune.lib.utils.setup_for_distributed">setup_for_distributed</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.warmup_lr_scheduler" href="#2_pytorch_finetune.lib.utils.warmup_lr_scheduler">warmup_lr_scheduler</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="2_pytorch_finetune.lib.utils.MetricLogger" href="#2_pytorch_finetune.lib.utils.MetricLogger">MetricLogger</a></code></h4>
<ul class="">
<li><code><a title="2_pytorch_finetune.lib.utils.MetricLogger.add_meter" href="#2_pytorch_finetune.lib.utils.MetricLogger.add_meter">add_meter</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.MetricLogger.log_every" href="#2_pytorch_finetune.lib.utils.MetricLogger.log_every">log_every</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.MetricLogger.synchronize_between_processes" href="#2_pytorch_finetune.lib.utils.MetricLogger.synchronize_between_processes">synchronize_between_processes</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.MetricLogger.update" href="#2_pytorch_finetune.lib.utils.MetricLogger.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue" href="#2_pytorch_finetune.lib.utils.SmoothedValue">SmoothedValue</a></code></h4>
<ul class="">
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.avg" href="#2_pytorch_finetune.lib.utils.SmoothedValue.avg">avg</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.global_avg" href="#2_pytorch_finetune.lib.utils.SmoothedValue.global_avg">global_avg</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.max" href="#2_pytorch_finetune.lib.utils.SmoothedValue.max">max</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.median" href="#2_pytorch_finetune.lib.utils.SmoothedValue.median">median</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.synchronize_between_processes" href="#2_pytorch_finetune.lib.utils.SmoothedValue.synchronize_between_processes">synchronize_between_processes</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.update" href="#2_pytorch_finetune.lib.utils.SmoothedValue.update">update</a></code></li>
<li><code><a title="2_pytorch_finetune.lib.utils.SmoothedValue.value" href="#2_pytorch_finetune.lib.utils.SmoothedValue.value">value</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>